{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text in charaters: 1115393\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the text in charaters: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "print(f'First {n} characters:\\n{text[:n]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building of the vocabulary based on the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(f'Vocabulary: {\"\".join(vocab)}')\n",
    "print(f'Vocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping of the characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctoi = {c:i for i,c in enumerate(vocab)}\n",
    "itoc = {i:c for i,c in enumerate(vocab)}\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: [itoc[i] for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: sample text\n",
      "Encoding example: [57, 39, 51, 54, 50, 43, 1, 58, 43, 62, 58]\n",
      "Decoding example: sample text\n"
     ]
    }
   ],
   "source": [
    "sample_text = 'sample text'\n",
    "print(f'Sample text: {sample_text}')\n",
    "print(f'Encoding example: {encode(sample_text)}')\n",
    "print(f'Decoding example: {\"\".join(decode(encode(sample_text)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding of the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: torch.Size([1115393])\n",
      "Data tensor content (first 50 elements): tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text))\n",
    "print(f'Data tensor shape: {data.shape}')\n",
    "print(f'Data tensor content (first 50 elements): {data[:50]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting of the dataset (training & validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in training set: 1003853\n",
      "Characters in validation set: 111540\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.9\n",
    "train_count = int(train_split * len(data))\n",
    "train_data = data[:train_count]\n",
    "val_data = data[train_count:]\n",
    "print(f'Characters in training set: {len(train_data)}')\n",
    "print(f'Characters in validation set: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structuring of the data splits into blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample block from training set: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "block_sample_train = train_data[:block_size+1]\n",
    "print(f'Sample block from training set: {block_sample_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structuring of the blocks into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training batch X shape: torch.Size([4, 8])\n",
      "Sample training batch Y shape: torch.Size([4, 8])\n",
      "Sample training batch X:\n",
      " tensor([[21, 17, 32, 10,  0, 27,  1, 58],\n",
      "        [ 6,  1, 44, 53, 53, 50, 47, 57],\n",
      "        [43, 56,  2,  1, 39, 58,  1, 39],\n",
      "        [53, 59, 56,  1, 43, 63, 43, 57]])\n",
      "Sample training batch Y:\n",
      " tensor([[17, 32, 10,  0, 27,  1, 58, 46],\n",
      "        [ 1, 44, 53, 53, 50, 47, 57, 46],\n",
      "        [56,  2,  1, 39, 58,  1, 39,  1],\n",
      "        [59, 56,  1, 43, 63, 43, 57,  1]])\n",
      "Sample training batch X (decoded):\n",
      " ['IET:\\nO t', ', foolis', 'er! at a', 'our eyes']\n",
      "Sample training batch Y (decoded):\n",
      " ['ET:\\nO th', ' foolish', 'r! at a ', 'ur eyes ']\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(data, batch_size, block_size)\n",
    "print(f'Sample training batch X shape: {xb.shape}')\n",
    "print(f'Sample training batch Y shape: {yb.shape}')\n",
    "print(f'Sample training batch X:\\n {xb}')\n",
    "print(f'Sample training batch Y:\\n {yb}')\n",
    "print(f'Sample training batch X (decoded):\\n {[\"\".join(decode(x)) for x in xb.tolist()]}')\n",
    "print(f'Sample training batch Y (decoded):\\n {[\"\".join(decode(y)) for y in yb.tolist()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of a simple Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(input=logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample inference through the Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch logits shape: torch.Size([32, 65])\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(f'Sample batch logits shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample generation of tokens using the Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting token (input for the model): ['\\n']\n",
      "Generated tokens: \n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLER\n"
     ]
    }
   ],
   "source": [
    "idx_start = torch.zeros(size=(1, 1), dtype=torch.long)\n",
    "idx_pred = m.generate(idx=idx_start, max_new_tokens=50)\n",
    "idx_pred_decoded = decode(idx_pred[0].tolist())\n",
    "print(f'Starting token (input for the model): {decode(idx_start[0].tolist())}')\n",
    "print(f'Generated tokens: {\"\".join(idx_pred_decoded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimization of the Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after training: 3.794015884399414\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "for step in range(1000):\n",
    "    xb, yb = get_batch(train_data, batch_size, block_size)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f'Loss after training: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting token (input for the model): ['\\n']\n",
      "Generated tokens: \n",
      "C xtRjRy!\n",
      "SAAd.\n",
      "wDWCASlvirTOq-onAGj;pJFq,Sb?suArdl\n"
     ]
    }
   ],
   "source": [
    "idx_start = torch.zeros(size=(1, 1), dtype=torch.long)\n",
    "idx_pred = m.generate(idx=idx_start, max_new_tokens=50)\n",
    "idx_pred_decoded = decode(idx_pred[0].tolist())\n",
    "print(f'Starting token (input for the model): {decode(idx_start[0].tolist())}')\n",
    "print(f'Generated tokens: {\"\".join(idx_pred_decoded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building up the self-attention mechanism (toy example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weighted aggregation through matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask matrix:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "Input matrix:\n",
      " tensor([[5., 1., 6., 5.],\n",
      "        [6., 4., 2., 5.],\n",
      "        [5., 9., 3., 1.],\n",
      "        [4., 2., 3., 2.],\n",
      "        [6., 8., 2., 2.],\n",
      "        [8., 2., 0., 4.],\n",
      "        [9., 2., 1., 9.],\n",
      "        [2., 2., 9., 4.]])\n",
      "Multiplication result matrix:\n",
      " tensor([[5.0000, 1.0000, 6.0000, 5.0000],\n",
      "        [5.5000, 2.5000, 4.0000, 5.0000],\n",
      "        [5.3333, 4.6667, 3.6667, 3.6667],\n",
      "        [5.0000, 4.0000, 3.5000, 3.2500],\n",
      "        [5.2000, 4.8000, 3.2000, 3.0000],\n",
      "        [5.6667, 4.3333, 2.6667, 3.1667],\n",
      "        [6.1429, 4.0000, 2.4286, 4.0000],\n",
      "        [5.6250, 3.7500, 3.2500, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "block_size = 8\n",
    "a = torch.tril(input=torch.ones(block_size, block_size))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(low=0, high=10, size=(block_size, 4)).float()\n",
    "c = a @ b\n",
    "print(f'Mask matrix:\\n {a}')\n",
    "print(f'Input matrix:\\n {b}')\n",
    "print(f'Multiplication result matrix:\\n {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example input initialization\n",
    "For using the self-attention mechanism, a simple example batch of token sequences is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape: torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # Number of blocks in a batch\n",
    "block_size = 8 # Number of tokens in a block (context)\n",
    "channels = 2 # Number of dimensions per token embedding\n",
    "B, T, C = batch_size, block_size, channels\n",
    "xb = torch.randn(size=(B, T, C))\n",
    "print(f'Sample batch shape: {xb.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words shape: torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = xb[b, :t+1]\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "print(f'Bag of words shape: {xbow.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 2 (basic normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output similar to previous version: True\n"
     ]
    }
   ],
   "source": [
    "wei = torch.tril(input=torch.ones(size=(T, T)))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow_v2 = wei @ xb\n",
    "print(f'Output similar to previous version: {torch.allclose(input=xbow_v2, other=xbow)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 3 (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output similar to previous version: True\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(input=torch.ones(size=(T, T)))\n",
    "wei = torch.zeros(size=(T, T))\n",
    "wei = torch.masked_fill(wei, tril == 0, float('-inf'))\n",
    "wei = F.softmax(input=wei, dim=-1)\n",
    "xbow_v3 = wei @ xb\n",
    "print(f'Output similar to previous version: {torch.allclose(input=xbow_v3, other=xbow_v2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 4 (attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 8, 16])\n",
      "Output content (0th batch):\n",
      " tensor([[ 0.2605,  1.3034,  0.6661,  0.9853, -1.2583, -0.3623, -0.8652,  0.3540,\n",
      "          0.6784, -0.3922,  0.4535,  0.2557,  0.1285, -0.1088,  0.6791,  0.4476],\n",
      "        [ 0.3575,  0.1789,  0.3514,  0.7294, -0.2864, -0.3199, -0.2846,  0.7067,\n",
      "         -0.2138,  0.3514,  0.5947,  0.2925,  0.5467, -0.3664,  0.2156,  0.3286],\n",
      "        [ 0.2889,  0.2966,  0.3371,  0.6484, -0.3675, -0.2753, -0.3153,  0.5503,\n",
      "         -0.0647,  0.2000,  0.4833,  0.2419,  0.4069, -0.2757,  0.2419,  0.2926],\n",
      "        [ 0.3745,  0.3949,  0.4407,  0.8444, -0.4857, -0.3580, -0.4145,  0.7119,\n",
      "         -0.0765,  0.2535,  0.6266,  0.3140,  0.5250, -0.3559,  0.3182,  0.3811],\n",
      "        [ 0.0806,  0.2555,  0.1544,  0.2476, -0.2571, -0.0958, -0.1848,  0.1297,\n",
      "          0.1048, -0.0397,  0.1377,  0.0737,  0.0737, -0.0535,  0.1443,  0.1122],\n",
      "        [ 0.2712,  0.2604,  0.3102,  0.6016, -0.3289, -0.2564, -0.2858,  0.5190,\n",
      "         -0.0735,  0.1977,  0.4534,  0.2264,  0.3861, -0.2612,  0.2190,  0.2714],\n",
      "        [-0.0758, -0.3600, -0.1871, -0.2793,  0.3489,  0.1033,  0.2410, -0.1057,\n",
      "         -0.1836,  0.1034, -0.1317, -0.0737, -0.0419,  0.0343, -0.1890, -0.1269],\n",
      "        [-0.1322, -0.3722, -0.2369, -0.3882,  0.3797,  0.1520,  0.2769, -0.2194,\n",
      "         -0.1385,  0.0391, -0.2252, -0.1193, -0.1318,  0.0943, -0.2159, -0.1759]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_head_size = 16\n",
    "query_layer = nn.Linear(in_features=C, out_features=attention_head_size, bias=False)\n",
    "key_layer = nn.Linear(in_features=C, out_features=attention_head_size, bias=False)\n",
    "value_layer = nn.Linear(in_features=C, out_features=attention_head_size, bias=False)\n",
    "query = query_layer(xb) # (B, T, C) = (4, 8, 16)\n",
    "key = key_layer(xb) # (B, T, C) = (4, 8, 16)\n",
    "wei = query @ key.transpose(-2, -1)\n",
    "tril = torch.tril(input=torch.ones(size=(T, T)))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(input=wei, dim=-1) # (B, T, T) = (4, 8, 8)\n",
    "value = value_layer(xb)\n",
    "out = wei @ value\n",
    "print(f'Output shape: {out.shape}')\n",
    "print(f'Output content (0th batch):\\n {out[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

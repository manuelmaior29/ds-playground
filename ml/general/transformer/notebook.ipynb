{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text in charaters: 1115393\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the text in charaters: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "print(f'First {n} characters:\\n{text[:n]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building of the vocabulary based on the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(f'Vocabulary: {\"\".join(vocab)}')\n",
    "print(f'Vocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping of the characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctoi = {c:i for i,c in enumerate(vocab)}\n",
    "itoc = {i:c for i,c in enumerate(vocab)}\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: [itoc[i] for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: sample text\n",
      "Encoding example: [57, 39, 51, 54, 50, 43, 1, 58, 43, 62, 58]\n",
      "Decoding example: sample text\n"
     ]
    }
   ],
   "source": [
    "sample_text = 'sample text'\n",
    "print(f'Sample text: {sample_text}')\n",
    "print(f'Encoding example: {encode(sample_text)}')\n",
    "print(f'Decoding example: {\"\".join(decode(encode(sample_text)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding of the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: torch.Size([1115393])\n",
      "Data tensor content (first 50 elements): tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text))\n",
    "print(f'Data tensor shape: {data.shape}')\n",
    "print(f'Data tensor content (first 50 elements): {data[:50]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting of the dataset (training & validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in training set: 1003853\n",
      "Characters in validation set: 111540\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.9\n",
    "train_count = int(train_split * len(data))\n",
    "train_data = data[:train_count]\n",
    "val_data = data[train_count:]\n",
    "print(f'Characters in training set: {len(train_data)}')\n",
    "print(f'Characters in validation set: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structuring of the data splits into blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample block from training set: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "block_sample_train = train_data[:block_size+1]\n",
    "print(f'Sample block from training set: {block_sample_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structuring of the blocks into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training batch X shape: torch.Size([4, 8])\n",
      "Sample training batch Y shape: torch.Size([4, 8])\n",
      "Sample training batch X:\n",
      " tensor([[21, 17, 32, 10,  0, 27,  1, 58],\n",
      "        [ 6,  1, 44, 53, 53, 50, 47, 57],\n",
      "        [43, 56,  2,  1, 39, 58,  1, 39],\n",
      "        [53, 59, 56,  1, 43, 63, 43, 57]])\n",
      "Sample training batch Y:\n",
      " tensor([[17, 32, 10,  0, 27,  1, 58, 46],\n",
      "        [ 1, 44, 53, 53, 50, 47, 57, 46],\n",
      "        [56,  2,  1, 39, 58,  1, 39,  1],\n",
      "        [59, 56,  1, 43, 63, 43, 57,  1]])\n",
      "Sample training batch X (decoded):\n",
      " ['IET:\\nO t', ', foolis', 'er! at a', 'our eyes']\n",
      "Sample training batch Y (decoded):\n",
      " ['ET:\\nO th', ' foolish', 'r! at a ', 'ur eyes ']\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(data, batch_size, block_size)\n",
    "print(f'Sample training batch X shape: {xb.shape}')\n",
    "print(f'Sample training batch Y shape: {yb.shape}')\n",
    "print(f'Sample training batch X:\\n {xb}')\n",
    "print(f'Sample training batch Y:\\n {yb}')\n",
    "print(f'Sample training batch X (decoded):\\n {[\"\".join(decode(x)) for x in xb.tolist()]}')\n",
    "print(f'Sample training batch Y (decoded):\\n {[\"\".join(decode(y)) for y in yb.tolist()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of a simple Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(input=logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample inference through the Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch logits shape: torch.Size([32, 65])\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(f'Sample batch logits shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample generation of tokens using the Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting token (input for the model): ['\\n']\n",
      "Generated tokens: \n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLER\n"
     ]
    }
   ],
   "source": [
    "idx_start = torch.zeros(size=(1, 1), dtype=torch.long)\n",
    "idx_pred = m.generate(idx=idx_start, max_new_tokens=50)\n",
    "idx_pred_decoded = decode(idx_pred[0].tolist())\n",
    "print(f'Starting token (input for the model): {decode(idx_start[0].tolist())}')\n",
    "print(f'Generated tokens: {\"\".join(idx_pred_decoded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimization of the Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after training: 3.794015884399414\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "for step in range(1000):\n",
    "    xb, yb = get_batch(train_data, batch_size, block_size)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f'Loss after training: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting token (input for the model): ['\\n']\n",
      "Generated tokens: \n",
      "C xtRjRy!\n",
      "SAAd.\n",
      "wDWCASlvirTOq-onAGj;pJFq,Sb?suArdl\n"
     ]
    }
   ],
   "source": [
    "idx_start = torch.zeros(size=(1, 1), dtype=torch.long)\n",
    "idx_pred = m.generate(idx=idx_start, max_new_tokens=50)\n",
    "idx_pred_decoded = decode(idx_pred[0].tolist())\n",
    "print(f'Starting token (input for the model): {decode(idx_start[0].tolist())}')\n",
    "print(f'Generated tokens: {\"\".join(idx_pred_decoded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building up the self-attention mechanism (toy example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weighted aggregation through matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask matrix:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "Input matrix:\n",
      " tensor([[5., 1., 6., 5.],\n",
      "        [6., 4., 2., 5.],\n",
      "        [5., 9., 3., 1.],\n",
      "        [4., 2., 3., 2.],\n",
      "        [6., 8., 2., 2.],\n",
      "        [8., 2., 0., 4.],\n",
      "        [9., 2., 1., 9.],\n",
      "        [2., 2., 9., 4.]])\n",
      "Multiplication result matrix:\n",
      " tensor([[5.0000, 1.0000, 6.0000, 5.0000],\n",
      "        [5.5000, 2.5000, 4.0000, 5.0000],\n",
      "        [5.3333, 4.6667, 3.6667, 3.6667],\n",
      "        [5.0000, 4.0000, 3.5000, 3.2500],\n",
      "        [5.2000, 4.8000, 3.2000, 3.0000],\n",
      "        [5.6667, 4.3333, 2.6667, 3.1667],\n",
      "        [6.1429, 4.0000, 2.4286, 4.0000],\n",
      "        [5.6250, 3.7500, 3.2500, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "block_size = 8\n",
    "a = torch.tril(input=torch.ones(block_size, block_size))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(low=0, high=10, size=(block_size, 4)).float()\n",
    "c = a @ b\n",
    "print(f'Mask matrix:\\n {a}')\n",
    "print(f'Input matrix:\\n {b}')\n",
    "print(f'Multiplication result matrix:\\n {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example input initialization\n",
    "For using the self-attention mechanism, a simple example batch of token sequences is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape: torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # Number of blocks in a batch\n",
    "block_size = 8 # Number of tokens in a block (context)\n",
    "channels = 2 # Number of dimensions per token embedding\n",
    "B, T, C = batch_size, block_size, channels\n",
    "xb = torch.randn(size=(B, T, C))\n",
    "print(f'Sample batch shape: {xb.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words shape: torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = xb[b, :t+1]\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "print(f'Bag of words shape: {xbow.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 2 (basic normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output similar to previous version: True\n"
     ]
    }
   ],
   "source": [
    "wei = torch.tril(input=torch.ones(size=(T, T)))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow_v2 = wei @ xb\n",
    "print(f'Output similar to previous version: {torch.allclose(input=xbow_v2, other=xbow)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 3 (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output similar to previous version: True\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(input=torch.ones(size=(T, T)))\n",
    "wei = torch.zeros(size=(T, T))\n",
    "wei = torch.masked_fill(wei, tril == 0, float('-inf'))\n",
    "wei = F.softmax(input=wei, dim=-1)\n",
    "xbow_v3 = wei @ xb\n",
    "print(f'Output similar to previous version: {torch.allclose(input=xbow_v3, other=xbow_v2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 4 (attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 8, 16])\n",
      "Output content (0th batch):\n",
      " tensor([[-0.4551, -0.5526,  0.4067,  0.2550, -0.3048, -0.1731, -0.5725,  0.1139,\n",
      "          0.2080,  0.5886,  1.4262,  0.2830,  0.5163, -0.3095, -1.2823, -0.7509],\n",
      "        [-0.5679, -0.4652,  0.6511,  0.4457, -0.3853,  0.0788, -0.7776, -0.2069,\n",
      "          0.3766,  0.5696,  1.0918,  0.1085,  0.5189, -0.2046, -1.0626, -0.4917],\n",
      "        [-0.5463, -0.3222,  0.7065,  0.4998, -0.3733,  0.2403, -0.7833, -0.3939,\n",
      "          0.4276,  0.4559,  0.6663, -0.0323,  0.4292, -0.0954, -0.7220, -0.2245],\n",
      "        [-0.5706, -0.2554,  0.7897,  0.5681, -0.3917,  0.3576, -0.8409, -0.5376,\n",
      "          0.4890,  0.4165,  0.4470, -0.1222,  0.4029, -0.0340, -0.5596, -0.0734],\n",
      "        [-0.5796, -0.3815,  0.7242,  0.5078, -0.3952,  0.2029, -0.8199, -0.3562,\n",
      "          0.4330,  0.5128,  0.8284,  0.0090,  0.4775, -0.1333, -0.8610, -0.3168],\n",
      "        [-0.6588, -0.5004,  0.7804,  0.5392, -0.4477,  0.1429, -0.9131, -0.3010,\n",
      "          0.4574,  0.6319,  1.1462,  0.0830,  0.5800, -0.2056, -1.1386, -0.4926],\n",
      "        [-0.7597, -0.5898,  0.8918,  0.6146, -0.5161,  0.1480, -1.0494, -0.3273,\n",
      "          0.5208,  0.7381,  1.3610,  0.1097,  0.6760, -0.2474, -1.3436, -0.5934],\n",
      "        [-0.4226, -0.2130,  0.5697,  0.4073, -0.2896,  0.2336, -0.6162, -0.3611,\n",
      "          0.3498,  0.3260,  0.4042, -0.0645,  0.3117, -0.0445, -0.4716, -0.1017]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_head_size = 16\n",
    "query_layer = nn.Linear(in_features=C, out_features=attention_head_size, bias=False)\n",
    "key_layer = nn.Linear(in_features=C, out_features=attention_head_size, bias=False)\n",
    "value_layer = nn.Linear(in_features=C, out_features=attention_head_size, bias=False)\n",
    "query = query_layer(xb) # (B, T, C) = (4, 8, 16)\n",
    "key = key_layer(xb) # (B, T, C) = (4, 8, 16)\n",
    "wei = query @ key.transpose(-2, -1)\n",
    "tril = torch.tril(input=torch.ones(size=(T, T)))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(input=wei, dim=-1) # (B, T, T) = (4, 8, 8)\n",
    "value = value_layer(xb)\n",
    "out = wei @ value\n",
    "print(f'Output shape: {out.shape}')\n",
    "print(f'Output content (0th batch):\\n {out[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaled attention (illustrative example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unscaled softmax: tensor([0.0695, 0.1548, 0.0028, 0.7666, 0.0063])\n",
      "Scaled softmax: tensor([0.2055, 0.2271, 0.1378, 0.2774, 0.1522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\AppData\\Local\\Temp\\ipykernel_37388\\2547081506.py:10: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  ax[0].set_yticks(torch.range(start=0.0, end=1.0, step=0.1))\n",
      "C:\\Users\\Manuel\\AppData\\Local\\Temp\\ipykernel_37388\\2547081506.py:11: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  ax[1].set_yticks(torch.range(start=0.0, end=1.0, step=0.1))\n",
      "C:\\Users\\Manuel\\AppData\\Local\\Temp\\ipykernel_37388\\2547081506.py:14: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "vector = [0.1, 0.2, -0.3, 0.4, -0.2]\n",
    "vector_scaled_softmax = torch.softmax(torch.tensor(vector), dim=-1)\n",
    "vector_unscaled_softmax = torch.softmax(torch.tensor(vector)*8, dim=-1)\n",
    "print(f'Unscaled softmax: {vector_unscaled_softmax}')\n",
    "print(f'Scaled softmax: {vector_scaled_softmax}')\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].bar(x=vector, height=vector_unscaled_softmax.tolist())\n",
    "ax[1].bar(x=vector, height=vector_scaled_softmax.tolist())\n",
    "ax[0].set_yticks(torch.range(start=0.0, end=1.0, step=0.1))\n",
    "ax[1].set_yticks(torch.range(start=0.0, end=1.0, step=0.1))\n",
    "ax[0].set_title('Unscaled softmax')\n",
    "ax[1].set_title('Scaled softmax')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining components for building a decoder-only transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layer normalization operation (LN)\n",
    "- Used to normalized the activations of a layer, across the entire layer, **independently for each example in a batch**\n",
    "- LN is performed by subtracting the mean from and dividing the activations by the standard deviation, computed across the **feature dimension** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1d:\n",
    "    def __init__(self, dim, epsilon=1e-5, momentum=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        var = x.var(dim=1, keepdim=True)\n",
    "        normalized = (x - mean) / torch.sqrt(var + self.epsilon)\n",
    "        self.out = self.gamma * normalized + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation (across features):\n",
      " tensor([0.9999, 0.9995, 0.9998, 0.9999, 0.9998, 1.0000, 1.0000, 0.9999])\n",
      "Mean (across features):\n",
      " tensor([-1.4901e-08, -1.6391e-07,  2.9802e-08, -8.9407e-08,  2.1607e-07,\n",
      "        -1.4901e-08, -4.4703e-08,  6.5193e-09])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(size=(8, 4))\n",
    "layer_norm = LayerNorm1d(dim=x.shape)\n",
    "x_layer_normalized = layer_norm(x)\n",
    "print(f'Standard deviation (across features):\\n {x_layer_normalized.std(dim=-1, keepdim=False)}')\n",
    "print(f'Mean (across features):\\n {x_layer_normalized.mean(dim=-1, keepdim=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer Decoder module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(in_features=embedding_dim, out_features=head_size, bias=False)\n",
    "        self.key = nn.Linear(in_features=embedding_dim, out_features=head_size, bias=False)\n",
    "        self.value = nn.Linear(in_features=embedding_dim, out_features=head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        _, T, _ = input.shape\n",
    "        q = self.query(input)\n",
    "        k = self.key(input)\n",
    "        v = self.value(input)\n",
    "        wei = q @ k.transpose(-2, -1) * (k.shape[-1]**-0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = torch.softmax(input=wei, dim=-1)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(in_features=num_heads*head_size, out_features=embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = torch.cat([head(input) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = embedding_dim // num_heads\n",
    "        self.self_attention = MultiHeadAttention(num_heads, self.head_size, embedding_dim)\n",
    "        self.feed_forward = FeedForward(embedding_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = input + self.self_attention(self.layer_norm1(input))\n",
    "        out = out + self.feed_forward(self.layer_norm1(out))\n",
    "        return out\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, num_embeddings, embedding_dim, num_heads, block_size, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def generate(self, input):\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
